{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dd7c4c87",
      "metadata": {
        "id": "dd7c4c87"
      },
      "source": [
        "# Building Agentic RAG with LlamaIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project implements an Agentic Retrieval-Augmented Generation (RAG) using LlamaIndex and OpenAI API to process and query information from a PDF document. The system is designed to have two query engines: summary_query_engine to summarize the document and qa_query_engine to answer specific questions.  The Router Query Engine selects dynamically the best query engine based on user input.\n",
        "\n",
        "# Pipeline:\n",
        "\n",
        "1. **Setup Environment & Install Dependencies**\n",
        "* Mount Google Drive\n",
        "* Install llama-index, openai, pypdf, etc.\n",
        "2. **Initialize OpenAI API & Models**\n",
        "* Define LLM and embedding model (GPT-4, text-embedding-ada-002).\n",
        "* Set up Settings.llm and Settings.embed_model.\n",
        "3. **Load and Process Documents**\n",
        "* Read pdf file (transformers.pdf in my  case) using SimpleDirectoryReader.\n",
        "* Convert document data into vector and summary indexes.\n",
        "4. **Create Query Engines**\n",
        "* summary_query_engine for summarization.\n",
        "* qa_query_engine for answering specific questions.\n",
        "5. **Configure the Router Query Engine**\n",
        "* Uses LLM-based query selection (LLMSingleSelector).\n",
        "* Dynamically routes the query to the correct engine.\n",
        "6. **Query the System**\n",
        "* User inputs a question.\n",
        "* The router selects the best query engine."
      ],
      "metadata": {
        "id": "jUXWbChTnY6z"
      },
      "id": "jUXWbChTnY6z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "This project is based on the course **\"Building Agentic RAG with Llamaindex\"** by **Deeplearning.AI** and is available at the following [link](https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/)."
      ],
      "metadata": {
        "id": "xyjLw1G3tnd_"
      },
      "id": "xyjLw1G3tnd_"
    },
    {
      "cell_type": "markdown",
      "id": "357c97c9",
      "metadata": {
        "id": "357c97c9"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NH8irGTCpUn",
        "outputId": "1cb5887c-eb47-4453-f453-f0d65e289df7"
      },
      "id": "6NH8irGTCpUn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"YOUR-PATH-HERE\""
      ],
      "metadata": {
        "id": "08WJHgrSwWTy"
      },
      "id": "08WJHgrSwWTy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install llama-index llama-index-llms-openai llama-index-embeddings-openai openai pypdf"
      ],
      "metadata": {
        "id": "2gUt0H5LH55X"
      },
      "id": "2gUt0H5LH55X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kayYqIZWHZib",
        "outputId": "3f883090-ecdb-45f2-f402-b31c5fe38083"
      },
      "id": "kayYqIZWHZib",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama-index                             0.12.19\n",
            "llama-index-agent-openai                0.4.6\n",
            "llama-index-cli                         0.4.0\n",
            "llama-index-core                        0.12.19\n",
            "llama-index-embeddings-openai           0.3.1\n",
            "llama-index-indices-managed-llama-cloud 0.6.7\n",
            "llama-index-llms-openai                 0.3.20\n",
            "llama-index-multi-modal-llms-openai     0.4.3\n",
            "llama-index-program-openai              0.3.1\n",
            "llama-index-question-gen-openai         0.3.0\n",
            "llama-index-readers-file                0.4.5\n",
            "llama-index-readers-llama-parse         0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SummaryIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    Settings\n",
        ")\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from llama_index.core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "vgXLHIhIMiQC"
      },
      "id": "vgXLHIhIMiQC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5f4f4a-5890-451c-8869-24606ef9f396",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "ab5f4f4a-5890-451c-8869-24606ef9f396"
      },
      "outputs": [],
      "source": [
        "# Set OpenAI API key\n",
        "import openai\n",
        "\n",
        "openai.api_key = 'YOUR-OPENAI-API-KEY-HERE'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "KnAhNgOUZnN0"
      },
      "id": "KnAhNgOUZnN0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize OpenAI models (LLM & Embedding)\n",
        "llm = OpenAI(model=\"gpt-4\", temperature=0)\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "npjtwSwLMyXy"
      },
      "id": "npjtwSwLMyXy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and parse the PDF document\n",
        "reader = SimpleDirectoryReader(input_files=[\"transformers.pdf\"])\n",
        "documents = reader.load_data()"
      ],
      "metadata": {
        "id": "JxNh6v4gM5V9"
      },
      "id": "JxNh6v4gM5V9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Vector Index (for Q&A)\n",
        "vector_index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "9gxgMTb1M-Ne"
      },
      "id": "9gxgMTb1M-Ne",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Summary Index\n",
        "summary_index = SummaryIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "FnF_D-82NDlg"
      },
      "id": "FnF_D-82NDlg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Query Engines\n",
        "\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        "    )\n",
        "qa_query_engine = vector_index.as_query_engine()\n"
      ],
      "metadata": {
        "id": "lNgGqZuCNMOE"
      },
      "id": "lNgGqZuCNMOE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to the document\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "qa_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=qa_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from the document.\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "qY_EPk-TY3eM"
      },
      "id": "qY_EPk-TY3eM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Router Query Engine with the selector\n",
        "\n",
        "router_query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        qa_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "wDvupA_MNMRW"
      },
      "id": "wDvupA_MNMRW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the query\n",
        "import textwrap\n",
        "query1 = \"What is the summary of the document?\"\n",
        "response = router_query_engine.query(query1)\n",
        "\n",
        "# Extract the response text\n",
        "response_text = str(response) if isinstance(response, str) else response.response\n",
        "\n",
        "# Wrap text to a readable width\n",
        "wrapped_response = textwrap.fill(response_text, width=80)\n",
        "\n",
        "# Print structured output\n",
        "print(\"=\" * 80)\n",
        "print(f\"**User Query:**\\n{query1}\\n\")\n",
        "print(\"=\" * 80)\n",
        "print(\"**Generated Response:**\\n\")\n",
        "print(wrapped_response)\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ3wv6P0SCt4",
        "outputId": "a0321d51-c2ff-4fc0-b185-44d6c57f2ce9"
      },
      "id": "VJ3wv6P0SCt4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for a summary of the document, which directly relates to choice 1 about summarization questions related to the document..\n",
            "\u001b[0m================================================================================\n",
            "**User Query:**\n",
            "What is the summary of the document?\n",
            "\n",
            "================================================================================\n",
            "**Generated Response:**\n",
            "\n",
            "The document is a research paper titled \"Attention Is All You Need\" by a team\n",
            "from Google Brain and Google Research. It introduces a new network architecture\n",
            "known as the Transformer, which is based solely on attention mechanisms,\n",
            "replacing the recurrent and convolutional layers typically used in sequence\n",
            "transduction models. The Transformer model has demonstrated superior performance\n",
            "in terms of quality, parallelizability, and training time compared to existing\n",
            "models. It has been tested on two machine translation tasks, English-to-German\n",
            "and English-to-French, and has achieved state-of-the-art results. The paper also\n",
            "explores the benefits of self-attention layers and presents experiments on\n",
            "English constituency parsing. The authors plan to extend the Transformer to\n",
            "handle different input and output modalities and to explore local, restricted\n",
            "attention mechanisms for handling large inputs and outputs. The code used for\n",
            "training and evaluating the models is available online.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the query\n",
        "query2=\"What is the self-attention?\"\n",
        "response = router_query_engine.query(query2)\n",
        "\n",
        "# Extract the response text\n",
        "response_text = str(response) if isinstance(response, str) else response.response\n",
        "\n",
        "# Wrap text to a readable width\n",
        "wrapped_response = textwrap.fill(response_text, width=80)\n",
        "\n",
        "# Print structured output\n",
        "print(\"=\" * 80)\n",
        "print(f\"**User Query:**\\n{query2}\\n\")\n",
        "print(\"=\" * 80)\n",
        "print(\"**Generated Response:**\\n\")\n",
        "print(wrapped_response)\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjNtykDJa4-q",
        "outputId": "797cb9b3-be14-4ec6-8d27-96e85596c8a3"
      },
      "id": "wjNtykDJa4-q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: The question 'What is the self-attention?' is asking for a specific context or definition from the document, not a summary of the document..\n",
            "\u001b[0m================================================================================\n",
            "**User Query:**\n",
            "What is the self-attention?\n",
            "\n",
            "================================================================================\n",
            "**Generated Response:**\n",
            "\n",
            "Self-attention, also known as scaled dot-product attention, is a mechanism where\n",
            "the input consists of queries and keys of dimension dk, and values of dimension\n",
            "dv. The dot products of the query with all keys are computed, each divided by\n",
            "√dk, and a softmax function is applied to obtain the weights on the values. This\n",
            "attention function is computed on a set of queries simultaneously, packed\n",
            "together into a matrix Q. The keys and values are also packed together into\n",
            "matrices K and V. The matrix of outputs is computed as: Attention(Q, K, V) =\n",
            "softmax(QKT √dk )V. This mechanism is faster and more space-efficient in\n",
            "practice, as it can be implemented using highly optimized matrix multiplication\n",
            "code.\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}