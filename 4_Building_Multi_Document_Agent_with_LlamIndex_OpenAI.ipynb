{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dd7c4c87",
      "metadata": {
        "id": "dd7c4c87"
      },
      "source": [
        "# Building a Multi-Document Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jUXWbChTnY6z",
      "metadata": {
        "id": "jUXWbChTnY6z"
      },
      "source": [
        "In this project we learn how to extend Agent to handle multiple documents and   increasing complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xyjLw1G3tnd_",
      "metadata": {
        "id": "xyjLw1G3tnd_"
      },
      "source": [
        "# References\n",
        "\n",
        "This project is based on the course **\"Building Agentic RAG with Llamaindex\"** by **Deeplearning.AI** and is available at the following [link](https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "357c97c9",
      "metadata": {
        "id": "357c97c9"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6NH8irGTCpUn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NH8irGTCpUn",
        "outputId": "c5f7e62b-4604-4d72-a7ac-539cf0e979c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"YOUR-PATH-HERE\""
      ],
      "metadata": {
        "id": "u4WCuvy42hIf"
      },
      "id": "u4WCuvy42hIf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2gUt0H5LH55X",
      "metadata": {
        "id": "2gUt0H5LH55X"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-index llama-index-llms-openai llama-index-embeddings-openai openai pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kayYqIZWHZib",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kayYqIZWHZib",
        "outputId": "16557093-ea90-4a69-d3cd-9add4e3660c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama-index                             0.12.19\n",
            "llama-index-agent-openai                0.4.6\n",
            "llama-index-cli                         0.4.0\n",
            "llama-index-core                        0.12.19\n",
            "llama-index-embeddings-openai           0.3.1\n",
            "llama-index-indices-managed-llama-cloud 0.6.7\n",
            "llama-index-llms-openai                 0.3.20\n",
            "llama-index-multi-modal-llms-openai     0.4.3\n",
            "llama-index-program-openai              0.3.1\n",
            "llama-index-question-gen-openai         0.3.0\n",
            "llama-index-readers-file                0.4.5\n",
            "llama-index-readers-llama-parse         0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vgXLHIhIMiQC",
      "metadata": {
        "id": "vgXLHIhIMiQC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SummaryIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    Settings\n",
        ")\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.core.tools import FunctionTool, QueryEngineTool\n",
        "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
        "from typing import List, Optional\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5f4f4a-5890-451c-8869-24606ef9f396",
      "metadata": {
        "id": "ab5f4f4a-5890-451c-8869-24606ef9f396"
      },
      "outputs": [],
      "source": [
        "# Set OpenAI API key\n",
        "import openai\n",
        "\n",
        "openai.api_key = 'YOUR-OPENAI_API-KEY-HERE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KnAhNgOUZnN0",
      "metadata": {
        "id": "KnAhNgOUZnN0"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nvoOWE9fQFBe",
      "metadata": {
        "id": "nvoOWE9fQFBe"
      },
      "source": [
        "## Setup an agent over 3 papersÂ¶"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading 3 papers\n",
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "]"
      ],
      "metadata": {
        "id": "f3MUmON8j_Bd"
      },
      "id": "f3MUmON8j_Bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "I3FybS5_QYIj",
      "metadata": {
        "id": "I3FybS5_QYIj"
      },
      "source": [
        "## Setup the Query Tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_doc_tools(\n",
        "    file_path: str,\n",
        "    name: str,\n",
        ") -> str:\n",
        "    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
        "\n",
        "    # load documents\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "    vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "    def vector_query(\n",
        "        query: str,\n",
        "        page_numbers: Optional[List[str]] = None\n",
        "    ) -> str:\n",
        "        \"\"\"Use to answer questions over a given paper.\n",
        "\n",
        "        Useful if you have specific questions over the paper.\n",
        "        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
        "\n",
        "        Args:\n",
        "            query (str): the string query to be embedded.\n",
        "            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE\n",
        "                if we want to perform a vector search\n",
        "                over all pages. Otherwise, filter by the set of specified pages.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        page_numbers = page_numbers or []\n",
        "        metadata_dicts = [\n",
        "            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
        "        ]\n",
        "\n",
        "        query_engine = vector_index.as_query_engine(\n",
        "            similarity_top_k=2,\n",
        "            filters=MetadataFilters.from_dicts(\n",
        "                metadata_dicts,\n",
        "                condition=FilterCondition.OR\n",
        "            )\n",
        "        )\n",
        "        response = query_engine.query(query)\n",
        "        return response\n",
        "\n",
        "\n",
        "    vector_query_tool = FunctionTool.from_defaults(\n",
        "        name=f\"vector_tool_{name}\",\n",
        "        fn=vector_query\n",
        "    )\n",
        "\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "    )\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        name=f\"summary_tool_{name}\",\n",
        "        query_engine=summary_query_engine,\n",
        "        description=(\n",
        "            f\"Useful for summarization questions related to {name}\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return vector_query_tool, summary_tool"
      ],
      "metadata": {
        "id": "82gpoVnYsunD"
      },
      "id": "82gpoVnYsunD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each paper into a tool\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ],
      "metadata": {
        "id": "Qm_wj0WGj_EP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e8c688-b58b-45c5-d345-fd419fbc4fef"
      },
      "id": "Qm_wj0WGj_EP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: selfrag.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a flat list of papers\n",
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ],
      "metadata": {
        "id": "LXDaKNnytYtK"
      },
      "id": "LXDaKNnytYtK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(initial_tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUZugOgMtYwa",
        "outputId": "26f130fa-18a0-4469-db64-5bf48c236ea2"
      },
      "id": "LUZugOgMtYwa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "KE93Vv2gtYyw"
      },
      "id": "KE93Vv2gtYyw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define agents\n",
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    initial_tools,\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ],
      "metadata": {
        "id": "9oyUWaAStY13"
      },
      "id": "9oyUWaAStY13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
        "    \"and then tell me about the evaluation results\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaP7v5hotsnQ",
        "outputId": "c516f10d-19df-464e-c313-d5ac73578042"
      },
      "id": "GaP7v5hotsnQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
            "=== Function Output ===\n",
            "PG19 test split\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
            "=== Function Output ===\n",
            "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. Additionally, the models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
            "=== LLM Response ===\n",
            "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
            "\n",
            "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. The models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czIx5OvGtsqY",
        "outputId": "6e4c2546-0988-4660-c020-41c8779084a4"
      },
      "id": "czIx5OvGtsqY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
            "=== Function Output ===\n",
            "Self-RAG is a framework that improves the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It trains a single arbitrary LM to adaptively retrieve passages, generate text informed by these passages, and critique its own output using special tokens called reflection tokens. This framework outperforms other LLMs and retrieval-augmented models on various tasks, demonstrating improved performance in terms of factuality, correctness, and fluency.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
            "=== Function Output ===\n",
            "LongLoRA is a comprehensive framework that incorporates shifted sparse attention (S2-Attn) and improved LoRA to extend the context length of large language models (LLMs) efficiently. It introduces the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) components, which focus on modeling relations between facial action units and providing local tampering supervision for forgery detection. LongLoRA demonstrates superior performance in forgery detection across datasets and manipulation methods, showcasing its effectiveness and generalization capabilities.\n",
            "=== LLM Response ===\n",
            "Self-RAG is a framework that improves the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It trains a single arbitrary LM to adaptively retrieve passages, generate text informed by these passages, and critique its own output using special tokens called reflection tokens. This framework outperforms other LLMs and retrieval-augmented models on various tasks, demonstrating improved performance in terms of factuality, correctness, and fluency.\n",
            "\n",
            "LongLoRA is a comprehensive framework that incorporates shifted sparse attention (S2-Attn) and improved LoRA to extend the context length of large language models (LLMs) efficiently. It introduces the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) components, which focus on modeling relations between facial action units and providing local tampering supervision for forgery detection. LongLoRA demonstrates superior performance in forgery detection across datasets and manipulation methods, showcasing its effectiveness and generalization capabilities.\n",
            "Self-RAG is a framework that improves the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It trains a single arbitrary LM to adaptively retrieve passages, generate text informed by these passages, and critique its own output using special tokens called reflection tokens. This framework outperforms other LLMs and retrieval-augmented models on various tasks, demonstrating improved performance in terms of factuality, correctness, and fluency.\n",
            "\n",
            "LongLoRA is a comprehensive framework that incorporates shifted sparse attention (S2-Attn) and improved LoRA to extend the context length of large language models (LLMs) efficiently. It introduces the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) components, which focus on modeling relations between facial action units and providing local tampering supervision for forgery detection. LongLoRA demonstrates superior performance in forgery detection across datasets and manipulation methods, showcasing its effectiveness and generalization capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup an agent over 7 papers"
      ],
      "metadata": {
        "id": "8728TMBmuQ67"
      },
      "id": "8728TMBmuQ67"
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
        "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
        "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"loftq.pdf\",\n",
        "    \"swebench.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "    \"zipformer.pdf\",\n",
        "    \"values.pdf\"\n",
        "]"
      ],
      "metadata": {
        "id": "JLJqJtYnu7Jy"
      },
      "id": "JLJqJtYnu7Jy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To download these papers, you might the code below ( the papers are also available in the repository):\n",
        "#for url, paper in zip(urls, papers):\n",
        "     #!wget \"{url}\" -O \"{paper}\""
      ],
      "metadata": {
        "id": "xElm9Dp2tsu8"
      },
      "id": "xElm9Dp2tsu8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl4B0LootsyP",
        "outputId": "2b9f02f7-7b42-40d6-9470-b0e4437b2593"
      },
      "id": "vl4B0LootsyP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: loftq.pdf\n",
            "Getting tools for paper: swebench.pdf\n",
            "Getting tools for paper: selfrag.pdf\n",
            "Getting tools for paper: zipformer.pdf\n",
            "Getting tools for paper: values.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Description:\n",
        "The large number of documents might cause several problems:\n",
        "1. The tools might not all fit in the prompt\n",
        "2. Costs and latency will increase because of the increased number of tokens in prompt\n",
        "3. LLM could get confused: the LLM might fail to choose the right tool, when the number of choices is too large\n",
        "\n",
        "### Solution:\n",
        "When a user asks a query, we perform Retrival Augmentation, not on the level of text, but on the level of tools.  We first retrieve a small set of relevant tools, and then feed the relevant tools to the agent reasoning prompt (instead of feeding all tools).\n",
        "\n"
      ],
      "metadata": {
        "id": "LyEIN5ABv14V"
      },
      "id": "LyEIN5ABv14V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extend the Agent with Tool Retrieval"
      ],
      "metadata": {
        "id": "QvsnyBPvyIAD"
      },
      "id": "QvsnyBPvyIAD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting all tools in a flat list\n",
        "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ],
      "metadata": {
        "id": "C-4n2Kjlou1g"
      },
      "id": "C-4n2Kjlou1g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LlamaIndex has high indexing capabilities over text documents.\n",
        "# Since these tools are Python objects, we need to convert and seriialize these objects to a string repesentation and back.\n",
        "# This is solved by the object index abstratction in LlamaIndex.\n",
        "# define an \"object\" index and retriever over these tools\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")"
      ],
      "metadata": {
        "id": "8hzBhKgZou32"
      },
      "id": "8hzBhKgZou32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ],
      "metadata": {
        "id": "TTEJ59n-ou7Y"
      },
      "id": "TTEJ59n-ou7Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = obj_retriever.retrieve(\n",
        "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
        ")"
      ],
      "metadata": {
        "id": "YDARD-BYzL8s"
      },
      "id": "YDARD-BYzL8s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s94XT8RTz1oS",
        "outputId": "41634316-e9b3-44fd-fc7a-f303fd99546a"
      },
      "id": "s94XT8RTz1oS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolMetadata(description='Useful for summarization questions related to metagpt', name='summary_tool_metagpt', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools[1].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj-2Ljia0AC3",
        "outputId": "edb016d2-6340-4ab6-c5f6-5c2ed8dc9460"
      },
      "id": "Lj-2Ljia0AC3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools[2].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52Idl3dozL_B",
        "outputId": "63502a05-d571-4cbd-b7ce-76b93cb8c7f5"
      },
      "id": "52Idl3dozL_B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolMetadata(description='Useful for summarization questions related to values', name='summary_tool_values', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the agents and adding system prompt(optional)\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_retriever,\n",
        "    llm=llm,\n",
        "    system_prompt=\"\"\" \\\n",
        "You are an agent designed to answer queries over a set of given papers.\n",
        "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
        "\n",
        "\"\"\",\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ],
      "metadata": {
        "id": "WJc4LPzBzMB4"
      },
      "id": "WJc4LPzBzMB4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison queries\n",
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used \"\n",
        "    \"in MetaGPT and compare it against SWE-Bench\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WZuK0nyzMEN",
        "outputId": "65c9f2af-af68-42c9-cfdc-b514325625f1"
      },
      "id": "9WZuK0nyzMEN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks covering a wide range of programming challenges such as creating games, developing programs, and building tools.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset used in SWE-Bench is constructed by collecting merged pull requests from the top 100 Python repositories based on PyPI downloads. It includes task instances that require model predictions to generate patches for codebase modifications, with specific criteria such as resolving issues and introducing new tests. The dataset is continuously updated to include new task instances based on pull requests created after the training date of language models.\n",
            "=== LLM Response ===\n",
            "The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks covering a wide range of programming challenges such as creating games, developing programs, and building tools.\n",
            "\n",
            "On the other hand, the evaluation dataset used in SWE-Bench is constructed by collecting merged pull requests from the top 100 Python repositories based on PyPI downloads. It includes task instances that require model predictions to generate patches for codebase modifications, with specific criteria such as resolving issues and introducing new tests. The dataset is continuously updated to include new task instances based on pull requests created after the training date of language models.\n",
            "The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks covering a wide range of programming challenges such as creating games, developing programs, and building tools.\n",
            "\n",
            "On the other hand, the evaluation dataset used in SWE-Bench is constructed by collecting merged pull requests from the top 100 Python repositories based on PyPI downloads. It includes task instances that require model predictions to generate patches for codebase modifications, with specific criteria such as resolving issues and introducing new tests. The dataset is continuously updated to include new task instances based on pull requests created after the training date of language models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare and contrast queries\n",
        "response = agent.query(\n",
        "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
        "    \"Analyze the approach in each paper first. \"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k451pO3rzMG0",
        "outputId": "90d4ec65-de25-452d-da45-b1827a1d72f1"
      },
      "id": "k451pO3rzMG0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"Analyzing the approach in the LongLoRA paper.\"}\n",
            "=== Function Output ===\n",
            "The approach introduced in the LongLoRA paper involves utilizing shifted sparse attention (S2-Attn) during training to approximate standard self-attention patterns, allowing for extending the context window of pre-trained models while maintaining minimal accuracy compromise. The paper also emphasizes the importance of trainable normalization and embedding layers in bridging the gap between low-rank adaptation (LoRA) and full fine-tuning for long context extension. Additionally, the LongLoRA method incorporates components such as the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) to aid in forgery detection by modeling relations between facial action units and generating challenging samples for model learning through tampering with AU-related regions. These components contribute to achieving state-of-the-art performance in evaluations across datasets and manipulations, with qualitative visualizations provided to aid in understanding the modifications made during the manipulation process.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_loftq with args: {\"input\": \"Analyzing the approach in the LoftQ paper.\"}\n",
            "=== Function Output ===\n",
            "The LoftQ paper introduces a novel quantization framework that combines quantization and low-rank approximation to approximate high-precision pre-trained weights. This approach aims to provide a better initialization for subsequent LoRA fine-tuning, leading to improved performance on downstream tasks. LoftQ has been shown to outperform existing quantization methods, especially in challenging low-bit scenarios, across various natural language understanding, question answering, summarization, and natural language generation tasks. The method is effective, robust, and demonstrates consistent performance gains compared to other approaches like QLoRA. LoftQ offers a light approach for downstream task adaptation, sidestepping the training cost associated with Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ) methods. LoftQ focuses on quantization techniques for model compression, specifically targeting the DeBERTaV3-base model. By utilizing low-rank adapters, LoftQ achieves efficient quantization to reduce memory usage during training and storage phases. The method demonstrates competitive performance compared to full finetuning, showcasing its effectiveness in compressing models while maintaining performance levels on various NLP tasks. LoftQ is also compared favorably to pruning methods in terms of compression efficiency and memory savings, highlighting its potential as a promising compression technique for neural network models.\n",
            "=== LLM Response ===\n",
            "The LongLoRA paper introduces the LongLoRA approach, which utilizes shifted sparse attention (S2-Attn) during training to extend the context window of pre-trained models while maintaining minimal accuracy compromise. The paper emphasizes the importance of trainable normalization and embedding layers in bridging the gap between low-rank adaptation (LoRA) and full fine-tuning for long context extension. Additionally, LongLoRA incorporates components like the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) for forgery detection by modeling relations between facial action units and generating challenging samples for model learning through tampering with AU-related regions. The method achieves state-of-the-art performance in evaluations across datasets and manipulations.\n",
            "\n",
            "On the other hand, the LoftQ paper introduces a quantization framework that combines quantization and low-rank approximation to approximate high-precision pre-trained weights. This framework aims to provide a better initialization for subsequent LoRA fine-tuning, leading to improved performance on downstream tasks. LoftQ outperforms existing quantization methods, especially in challenging low-bit scenarios, across various natural language understanding, question answering, summarization, and natural language generation tasks. The method is effective, robust, and demonstrates consistent performance gains compared to other approaches like QLoRA. LoftQ offers a light approach for downstream task adaptation, avoiding the training cost associated with Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ) methods. It focuses on quantization techniques for model compression, specifically targeting the DeBERTaV3-base model, and achieves efficient quantization using low-rank adapters to reduce memory usage during training and storage phases. LoftQ demonstrates competitive performance compared to full fine-tuning and pruning methods, showcasing its effectiveness as a compression technique for neural network models.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}